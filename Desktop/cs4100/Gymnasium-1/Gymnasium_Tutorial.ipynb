{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac53cf9-7bfa-4da8-9506-c2dba2dda92e",
   "metadata": {},
   "source": [
    "Lab : Intro to Gymnasium\n",
    "----\n",
    "\n",
    "As we are steadily advancing towards Reinforcement Learning in class, it's time to get our hands dirty with a cool Python package. Gymnasium, originally developed by OpenAI is a useful tool that is commonly used for solving RL-related challenges. With Gymnasium, users can easily design, implement, and evaluate reinforcement learning algorithms by leveraging predefined environments such as classic control tasks, Atari 2600 games, and more. Furthermore, Gymnasium provides an easy API to implement your own environments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ed948-05a8-4a3f-9a66-c2a754c8b329",
   "metadata": {},
   "source": [
    "First, let's get started with installing Gymnasium!\n",
    "\n",
    "Step 1. Make sure you have python and pip installed. Check the installation of python and pip using\n",
    "- `python --version` and `pip --version`\n",
    "If pip is not installed:\n",
    "- Follow the steps listed for your respective operating system at [Pip Docs - Installation](https://pip.pypa.io/en/stable/installation/).\n",
    "\n",
    "Step 2. Now execute `pip install gym` in your terminal to install gymnasium or run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b3d3a92-0ace-4d2b-9389-93d9fd4699f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/anaconda3/envs/cs4100/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/envs/cs4100/lib/python3.11/site-packages (from gym) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/cs4100/lib/python3.11/site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /opt/anaconda3/envs/cs4100/lib/python3.11/site-packages (from gym) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaff9a-c81d-4889-9236-b65341252f47",
   "metadata": {},
   "source": [
    "Now to verify installation of gym execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7d2e68-590a-450e-8e64-a031afabe535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c4595-a2f2-4995-a65f-74d48bb57b48",
   "metadata": {},
   "source": [
    "Great! Now let's talk about environments!\n",
    "\n",
    "The fundamental building block of OpenAI Gym is the `Env` class. It is a Python class that implements a simulator that runs the  environment you want to train your agent in. Open AI Gym comes packed with a lot of environments, such as one where you can move a car up a hill, balance a swinging pendulum etc. Take a moment now to visit [Gymnasium - Farama](https://gymnasium.farama.org/) to see the complete list of environments, along with demonstrations of what the task in each environment is.\n",
    "\n",
    "In this lab, we will begin with the `MountainCar` environment. Our goal is to control a car on a track positioned between two mountains. The objective is to drive up the mountain on the right, but the car's engine lacks the power to climb it directly. To solve this problem, we require the car to make strategic back-and-forth movements to build momentum.\n",
    "\n",
    "Let's see how we can train our car to climb up the mountain using gymnasium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c64f01-9657-4071-a037-71919d671119",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb0da5d9-ba4b-4764-accf-1ca1e05948f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While creating our mountain car environment, render_mode is an optional argument.\n",
    "# It is used to render the environment visually when needed.\n",
    "# For more details, visit https://gymnasium.farama.org/api/env/\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7abdf-d9b9-42b4-9a8c-2efa1997704d",
   "metadata": {},
   "source": [
    "In Reinforcement Learning, we think of two key terms - the `Observation Space` and `Action space`.\n",
    "\n",
    "The `Observation Space` is the set of all possible observations that an agent can receive from the environment. These observations provide information about the current state of the environment. Observations can be diverse and may include sensor readings, images, or any relevant data that helps the agent make decisions.\n",
    "\n",
    "For example, for an agent trying to learn how to shoot a target the observation space can include - the agent's current position or location, the position and movement of the target, information about obstacles or barriers in the environment if any, and the agent's ammunition status or the number of bullets remaining.\n",
    "\n",
    "The `Action Space` represents the set of possible actions that the agent can take to interact with the environment. These actions represent the decisions or movements that the agent can make to interact with the environment.\n",
    "\n",
    "For example for an agent trying to learn how to shoot a target the action space can include - adjusting the aim or direction of the firearm, pulling the trigger to shoot, reloading the firearm, and changing the stance or position of the agent.\n",
    "\n",
    "The basic structure of the environment is described by the observation_space and the action_space attributes of the Gym Env class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cd8d0-0dfc-4d21-b5f4-48f74d1bb7a6",
   "metadata": {},
   "source": [
    "## To-do\n",
    "Can you guess what the observation space and action space of the mountain car problem would be? Write your answer below. Try to provide 2-3 examples for each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f0ae5a-1466-47a2-9a0f-fe36d6511653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR ANSWER HERE\n",
    "\n",
    "### Observation space: The car's potential energy, velocity, and position\n",
    "\n",
    "### Action space: Accelerate left, Accelerate right, Do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d161a237-b07c-4228-8f41-e422a45ccc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "The action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "# Observation and action space for mountain-view problem\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e1260-0405-497b-b1a8-3c4033200966",
   "metadata": {},
   "source": [
    "The observation_space for our environment is a Box with shape (2,), and returns two lists with entries of type float32.\n",
    "The action_space was Discrete with shape (2).\n",
    "\n",
    "What do these actually mean?\n",
    "\n",
    "Both Box and Discrete are types of data structures called \"Spaces\" provided by Gym, modeling various aspects of the problem at hand. All of these data structures are derived from the gym.Space base class.\n",
    "\n",
    "Box, for instance, is used when modeling real-valued quantities, i.e. a continuous space. The printed output can be a bit tricky to read - the first list in the observation space above contains minimum values for the set of quantities that are modeled, and the second list contains the maxima.\n",
    "\n",
    "For this particular problem (Mountain Car), the values at index 0 within both lists refer to the minimum and maximum value of the car's position along the x-axis, i.e. the car will always be between x=-1.2 and x=0.6. The values at index 1 represent the range of the velocity, i.e. the car's speed will be between -0.07 and 0.07. Both these values are allowed to vary continuously with the precision of the float32 dtype.\n",
    "\n",
    "Discrete, on the other hand, signifies that there are a finite number of options to choose from. In this case, Discrete(3) represents the fact that the agent has three choices: 0: accelerate left, 1: don't accelerate, and 2: accelerate right. The easiest way to find descriptions for actions (and their indices) is to visit the [Gymnasium page for that task](https://gymnasium.farama.org/environments/classic_control/mountain_car/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65911af6-c32d-45bc-aa78-a5257e3b9195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Bound for Env Observation [0.6  0.07]\n",
      "Lower Bound for Env Observation [-1.2  -0.07]\n"
     ]
    }
   ],
   "source": [
    "print(\"Upper Bound for Env Observation\", env.observation_space.high)\n",
    "print(\"Lower Bound for Env Observation\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0d771-dee2-4571-b337-1abb978f9549",
   "metadata": {},
   "source": [
    "You can set these upper/lower limits while defining your space, as well as when you are creating an environment.\n",
    "\n",
    "From here on, remember that an observation for the mountain car environment is a vector of two numbers, representing position and velocity respectively. The middle point between the two mountains in the environment is taken to be the origin, with the right being considered the positive direction and the left being the negative direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02de71b-4878-4222-ba60-262972798b87",
   "metadata": {},
   "source": [
    "Now let's talk about how the env class helps the agent interact with the environment!\n",
    "\n",
    "`reset`: This function returns the initial observation of the environment after placing the agent back in its starting state. Our first observation in any RL task implemented using Gymnasium must be obtained by calling this function. This function should also be called every time a terminal/end state is reached.\n",
    "`step`: This function takes an action as input, applies it to the environment, and returns the following:\n",
    "- `observation`: The current state of the environment after the action is taken.\n",
    "- `reward`: The reward obtained from the action.\n",
    "- `terminated`: Return true or false, depending on whether the agent has reached a terminal state (as defined under the task).\n",
    "- `truncated`: Determine if the truncation condition, often a time limit or agent going out of bounds, is satisfied. This can prompt an early episode termination before reaching a terminal state. Think of this as cases that may end a game preemptively.\n",
    "- `info`: Extra information for debugging or environment-specific details, such as lives remaining.\n",
    "\n",
    "Next, we demonstrate the step function, which is how we get the agent to execute an action in a Gymnasium environment. For now, we will simply pick a random action from the environment's action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1898f7c-34f7-4501-abb2-acc459a90bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial observation is (array([-0.5630212,  0.       ], dtype=float32), {})\n",
      "The new observation is [-5.627262e-01  2.949792e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs4100/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# reset the environment and see the initial observation\n",
    "obs = env.reset()\n",
    "print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "# Sample a random action from the entire action space\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# # Take the action and get the new observation space\n",
    "new_obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "print(\"The new observation is {}\".format(new_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3e61c01-ae9a-4d66-923e-b77aa005d63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pygame 2.6.1\n",
      "Uninstalling pygame-2.6.1:\n",
      "  Successfully uninstalled pygame-2.6.1\n",
      "Collecting pygame\n",
      "  Using cached pygame-2.6.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Using cached pygame-2.6.1-cp311-cp311-macosx_11_0_arm64.whl (12.4 MB)\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n",
      "Requirement already satisfied: gym[classic_control] in /opt/anaconda3/envs/cs4100/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/envs/cs4100/lib/python3.11/site-packages (from gym[classic_control]) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/cs4100/lib/python3.11/site-packages (from gym[classic_control]) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /opt/anaconda3/envs/cs4100/lib/python3.11/site-packages (from gym[classic_control]) (0.1.0)\n",
      "Collecting pygame==2.1.0 (from gym[classic_control])\n",
      "  Using cached pygame-2.1.0.tar.gz (5.8 MB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25lerror\n",
      "  \u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\n",
      "  \n",
      "  \u001B[31m×\u001B[0m \u001B[32mpython setup.py egg_info\u001B[0m did not run successfully.\n",
      "  \u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\n",
      "  \u001B[31m╰─>\u001B[0m \u001B[31m[31 lines of output]\u001B[0m\n",
      "  \u001B[31m   \u001B[0m \n",
      "  \u001B[31m   \u001B[0m \n",
      "  \u001B[31m   \u001B[0m WARNING, No \"Setup\" File Exists, Running \"buildconfig/config.py\"\n",
      "  \u001B[31m   \u001B[0m Using Darwin configuration...\n",
      "  \u001B[31m   \u001B[0m \n",
      "  \u001B[31m   \u001B[0m /bin/sh: sdl2-config: command not found\n",
      "  \u001B[31m   \u001B[0m /bin/sh: sdl2-config: command not found\n",
      "  \u001B[31m   \u001B[0m /bin/sh: sdl2-config: command not found\n",
      "  \u001B[31m   \u001B[0m \n",
      "  \u001B[31m   \u001B[0m ---\n",
      "  \u001B[31m   \u001B[0m For help with compilation see:\n",
      "  \u001B[31m   \u001B[0m     https://www.pygame.org/wiki/MacCompile\n",
      "  \u001B[31m   \u001B[0m To contribute to pygame development see:\n",
      "  \u001B[31m   \u001B[0m     https://www.pygame.org/contribute.html\n",
      "  \u001B[31m   \u001B[0m ---\n",
      "  \u001B[31m   \u001B[0m \n",
      "  \u001B[31m   \u001B[0m Traceback (most recent call last):\n",
      "  \u001B[31m   \u001B[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001B[31m   \u001B[0m   File \"<pip-setuptools-caller>\", line 35, in <module>\n",
      "  \u001B[31m   \u001B[0m   File \"/private/var/folders/p6/8f48hsn176503d_vqvf_hc800000gn/T/pip-install-7ys48pc3/pygame_d8aaccdbf83f4aef898c0a97738c5283/setup.py\", line 388, in <module>\n",
      "  \u001B[31m   \u001B[0m     buildconfig.config.main(AUTO_CONFIG)\n",
      "  \u001B[31m   \u001B[0m   File \"/private/var/folders/p6/8f48hsn176503d_vqvf_hc800000gn/T/pip-install-7ys48pc3/pygame_d8aaccdbf83f4aef898c0a97738c5283/buildconfig/config.py\", line 234, in main\n",
      "  \u001B[31m   \u001B[0m     deps = CFG.main(**kwds)\n",
      "  \u001B[31m   \u001B[0m            ^^^^^^^^^^^^^^^^\n",
      "  \u001B[31m   \u001B[0m   File \"/private/var/folders/p6/8f48hsn176503d_vqvf_hc800000gn/T/pip-install-7ys48pc3/pygame_d8aaccdbf83f4aef898c0a97738c5283/buildconfig/config_darwin.py\", line 132, in main\n",
      "  \u001B[31m   \u001B[0m     [DependencyProg('SDL', 'SDL_CONFIG', 'sdl2-config', '2.0', ['sdl'])],\n",
      "  \u001B[31m   \u001B[0m      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001B[31m   \u001B[0m   File \"/private/var/folders/p6/8f48hsn176503d_vqvf_hc800000gn/T/pip-install-7ys48pc3/pygame_d8aaccdbf83f4aef898c0a97738c5283/buildconfig/config_unix.py\", line 39, in __init__\n",
      "  \u001B[31m   \u001B[0m     self.ver = config[0].strip()\n",
      "  \u001B[31m   \u001B[0m                ~~~~~~^^^\n",
      "  \u001B[31m   \u001B[0m IndexError: list index out of range\n",
      "  \u001B[31m   \u001B[0m \u001B[31m[end of output]\u001B[0m\n",
      "  \n",
      "  \u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001B[?25h\u001B[1;31merror\u001B[0m: \u001B[1mmetadata-generation-failed\u001B[0m\n",
      "\n",
      "\u001B[31m×\u001B[0m Encountered error while generating package metadata.\n",
      "\u001B[31m╰─>\u001B[0m See above for output.\n",
      "\n",
      "\u001B[1;35mnote\u001B[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001B[1;36mhint\u001B[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "# This step is optional and only needs to be executed if you want to render and view the environment.\n",
    "# May be finicky on MacOS. It's fine to skip it for now!\n",
    "!pip uninstall -y pygame\n",
    "!pip install pygame --pre\n",
    "!pip install 'gym[classic_control]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28d9d259-6933-420a-8201-a536429bec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "# only way to end a simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1cd30-8d74-4d44-8bbf-422034a674af",
   "metadata": {},
   "source": [
    "# To-do\n",
    "Now it's your turn to try writing code to see how far up the hill this cart can go by taking random steps in this environment.\\\n",
    "**Since we are not training the agent at all, we don't expect to see much progress**, but this should give us a good idea of how gymnasium works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e95af7-3928-4a67-96f0-042eb1813e73",
   "metadata": {},
   "source": [
    "#### To Ponder:\n",
    "\n",
    "In the previous code block where we were taking actions, we didn't use the agent's state and reward to decide the best action from the new state. All the generated actions are random, and you'll be doing the same thing below. We'll learn how to train agents in lecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef06417-1720-42fd-9045-0cef49e52bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "env_screen= None\n",
    "\n",
    "# Number of steps you run the agent for\n",
    "# This may take a few minutes to execute depending on the number of steps\n",
    "num_steps = 2000\n",
    "\n",
    "# reset the environment here\n",
    "obs = env.reset()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Render the env here (Rendering might take longer for the cell to execute.\n",
    "    # Stop execution once you get a sense of what's happening and move on to the next part of the lab).\n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    # take random action (based on examples above)\n",
    "    random_action = env.action_space.sample()\n",
    "\n",
    "    # apply the action using env.step here\n",
    "    new_obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "\n",
    "    # If the episode is finished (i.e., terminated or truncated), then reset the env and start another one;\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "        break\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8741d2e0-62c9-4884-92ea-c7eec4d23900",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now that we have seen how to use gymnasium using its default environment, let's see how can we build a custom environment and perform actions in it. This particular custom environment is built using the gymnasium extension for Markov Decision Processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744f4c8-a757-45a1-9e64-627d1d88a589",
   "metadata": {},
   "source": [
    "Let's consider a 3x3 grid where the action space is to move up, down, left, or right. An action that takes you out of the grid is considered invalid. The red cells in the grid represent a reward of -20, the yellow cell has a reward of +5 and the green one has a reward of +10. Consider the green cell as the final goal state, which is where we want our agent to go. There is an equal probability of taking any valid action from a given cell.\n",
    "\n",
    "Here's how the grid is defined **(on a Euclidean plane)**:\n",
    "\n",
    "Cell 0 - 0,0 (start)\\\n",
    "Cell 1 - 0,1\\\n",
    "Cell 2 - 0,2\\\n",
    "Cell 3 - 1,0\\\n",
    "Cell 4 - 1,1\\\n",
    "Cell 5 - 1,2\\\n",
    "Cell 6 - 2,0 (goal)\\\n",
    "Cell 7 - 2,1\\\n",
    "Cell 8 - 2,2\n",
    "\n",
    "<div><img src=\"./grid.png\", style=\"width:400px\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e21d5-cf0c-48ca-a5ec-5b1be49ed96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matrix-mdp-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e5434-274a-4e43-8424-a8543f97b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_states = 9 # total number of cells\n",
    "num_actions = 4 # up, down, left, right\n",
    "num_terminal_states = 1 # initilaise the number of terminal goal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758daba-44c3-4f45-b859-7f5958185ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To-do:\n",
    "# Initialise the action map as a dictionary, just to save human readable labels. 0 maps to 'up', 1 to 'down', 2 to 'left', and 3 to 'right'.\n",
    "A = {\n",
    "    0: 'up',\n",
    "    1: 'down',\n",
    "    2: 'left',\n",
    "    3: 'right'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8bc81-5b56-498d-bea9-41891a5f5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-do: fill in and complete the state_action_map here, this is a map of all the valid actions from a given state.\n",
    "# Values for cell 0 and 6 are filled in for you\n",
    "# (from cell 0 we can go to the cell above it, or the cell to its right.\n",
    "# The action map values, therefore, are 0 and 3. For cell 6, since this state is terminal, there are no actions from it.\n",
    "\n",
    "# While we are hardcoding this for now, you should consider writing code\n",
    "# that generates this map using the rules of your environment in general.\n",
    "\n",
    "states_actions_map = {\n",
    "    0: [0, 3],\n",
    "    # COMPLETE THE MAP\n",
    "    1: [0, 2, 3],\n",
    "    2: [0, 2],\n",
    "    3: [0, 1, 3],\n",
    "    4: [0, 1, 2, 3],\n",
    "    5: [0, 1, 2],\n",
    "    7: [1, 2, 3],\n",
    "    8: [1, 2],\n",
    "    6: [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "id": "3a1f95d0-b87f-4737-b64f-29ada8813b84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T20:12:26.931517Z",
     "start_time": "2025-09-19T20:12:26.904808Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Initialize transition and reward matrices.\n",
    "states_actions_map = {\n",
    "    0: [0, 3],\n",
    "    # COMPLETE THE MAP\n",
    "    1: [0, 2, 3],\n",
    "    2: [0, 2],\n",
    "    3: [0, 1, 3],\n",
    "    4: [0, 1, 2, 3],\n",
    "    5: [0, 1, 2],\n",
    "    7: [1, 2, 3],\n",
    "    8: [1, 2],\n",
    "    6: [],\n",
    "}\n",
    "\n",
    "num_states = 9\n",
    "num_actions = 4\n",
    "\n",
    "T = np.zeros((num_states, num_states, num_actions))\n",
    "R = np.zeros((num_states, num_states, num_actions))\n",
    "\n",
    "# 2. Fill in the transition matrix with the correct values.\n",
    "\n",
    "    # The matrix T represents transition probabilities, i.e., the probability of moving to state j from state i using action a.\n",
    "    # In uncertain environments, an action may have a small probability of failing and leading you to a different state than intended.\n",
    "    # For now, assume that the probability of failure is 0, i.e., an up action, for instance,\n",
    "    # will always take you to the cell above your current state, as long as such a cell exists.\n",
    "\n",
    "    # Note that T is a 3-dimensional matrix. Entries should be made at the index [new_state][current_state][action].\n",
    "    # For example, T[1][0][3] = 1 means that if we take action 3 (right) from cell 0 (start), the probability of reaching the cell 1 is 1.\n",
    "    # Use numpy operations to fill in the correct values in the matrix T.\n",
    "\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    \"\"\"\n",
    "    returns the next state given current state and action\n",
    "    returns -1 if action is invalid\n",
    "    \"\"\"\n",
    "    # convert state number to (row, col) coord\n",
    "    row = state // 3\n",
    "    col = state % 3\n",
    "\n",
    "\n",
    "    if action == 0:  # up\n",
    "        new_row, new_col = row + 1, col\n",
    "    elif action == 1:  # down\n",
    "        new_row, new_col = row - 1, col\n",
    "    elif action == 2:  # left\n",
    "        new_row, new_col = row, col - 1\n",
    "    elif action == 3:  # right\n",
    "        new_row, new_col = row, col + 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "    # check bounds\n",
    "    if new_row < 0 or new_row >= 3 or new_col < 0 or new_col >= 3:\n",
    "        return -1\n",
    "\n",
    "    # convert back to state number\n",
    "    return new_row * 3 + new_col\n",
    "\n",
    "# initialize transition and reward matrices\n",
    "T = np.zeros((num_states, num_states, num_actions))\n",
    "R = np.zeros((num_states, num_states, num_actions))\n",
    "\n",
    "# fill in the transition matrix\n",
    "for current_state in range(num_states):\n",
    "    # skip terminal state\n",
    "    if current_state == 6:\n",
    "        continue\n",
    "\n",
    "    valid_actions = states_actions_map[current_state]\n",
    "\n",
    "    for action in range(num_actions):\n",
    "        if action in valid_actions:\n",
    "            next_state = get_next_state(current_state, action)\n",
    "            if next_state != -1:\n",
    "                T[next_state][current_state][action] = 1.0\n",
    "\n",
    "\n",
    "# 3. Fill in the reward matrix with the correct values.\n",
    "\n",
    "    # The matrix R represents the rewards obtained by moving to state j from state i using action a.\n",
    "    # The red cells in the grid have a reward of -20, the yellow cell has a reward of +5, and the green one has a reward of +10.\n",
    "    # Assume rewards don't depend on the action used or the cell from which the agent moved to one of these cells for now.\n",
    "\n",
    "    # Entries should once again be made at the index [new_state][current_state][action].\n",
    "    # For example, R[2][1][3] = -20 indicates that if we take action 3 from cell 1 and reach cell 2, the reward is -20.\n",
    "    # Update the entries in the matrix R with the correct values below.\n",
    "\n",
    "cell_rewards = {\n",
    "    0: 0,   # start cell - no reward\n",
    "    1: 0,   # neutral cell\n",
    "    2: -20, # red cell\n",
    "    3: 0,   # neutral cell\n",
    "    4: 0,   # neutral cell\n",
    "    5: 5,   # yellow cell\n",
    "    6: 10,  # green cell (goal)\n",
    "    7: -20, # red cell\n",
    "    8: -20  # red cell\n",
    "}\n",
    "\n",
    "# fill reward matrix\n",
    "for current_state in range(num_states):\n",
    "    for next_state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            if T[next_state][current_state][action] > 0:  # if transition is possible\n",
    "                R[next_state][current_state][action] = cell_rewards[next_state]\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3752407002.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mP_0 =\u001B[39m\n          ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m invalid syntax\n"
     ]
    }
   ],
   "execution_count": null,
   "source": "\n",
   "id": "1dca8165"
  },
  {
   "cell_type": "code",
   "id": "6aa0769d-4d9a-461f-beb0-1e00cf8dd1b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T20:12:35.167702Z",
     "start_time": "2025-09-19T20:12:35.156556Z"
    }
   },
   "source": [
    "# P_0 is simply the initial probability distribution. This represents where your agent is at the beginning.\n",
    "# If there is only one possible start state, the probability distribution is simply a one-hot vector,\n",
    "# where the probability at the start state is 1 and 0 elsewhere.\n",
    "\n",
    "import numpy as np\n",
    "# Start state is cell 0 (0,0)\n",
    "P_0 = np.zeros(num_states)\n",
    "P_0[0] = 1  # Agent starts at cell 0 with probability 1"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "515f2f0d-09b1-4bad-9163-73c901ce7d7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T20:12:52.595252Z",
     "start_time": "2025-09-19T20:12:51.459458Z"
    }
   },
   "source": [
    "import matrix_mdp\n",
    "import gymnasium as gym\n",
    "env = gym.make('matrix_mdp/MatrixMDP-v0', p_0=P_0, p=T, r=R)\n",
    "\n",
    "#First, we reset the environment and get the initial observation.\n",
    "observation, info = env.reset()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "ca894307-9976-4215-bc81-c751fc00d66d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T20:12:38.314078Z",
     "start_time": "2025-09-19T20:12:38.264048Z"
    }
   },
   "source": [
    "#First, we reset the environment and get the initial observation.\n",
    "observation, info = env.reset()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#First, we reset the environment and get the initial observation.\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m observation, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'env' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "b07c9c72-19ec-4995-828f-a74b7a32ed3b",
   "metadata": {},
   "source": [
    "Below, write the code for random exploration, i.e. randomly choosing an action at each time step and executing it.\n",
    "\n",
    "A random action is simply a random integer between 0 and the number of actions (num_actions not inclusive).\n",
    "However, you should make sure that the chosen action can actually be taken from the current state (i.e., the chosen action is valid).\n",
    "\n",
    "Keep track of the total reward in each episode, and reset the environment when the episode terminates.\n",
    "\n",
    "Print the average reward obtained over 10000 episodes.\n",
    "\n",
    "**Once again, since we are not training the agent at all, you should expect to see large negative rewards.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d1662-ba33-4f01-aec6-88be1cfa8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting total reward as zero\n",
    "total_rewards = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d33a4a4-8b6b-452e-bc71-ec6adfdda8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T20:16:09.106452Z",
     "start_time": "2025-09-19T20:16:06.413717Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "# To-Do:\n",
    "# For each episode, collect the reward the agent earns and store it.\n",
    "# After the end of all 10000 episodes, we are going to take the average reward earned by the agent.\n",
    "\n",
    "reward_list = []\n",
    "\n",
    "for i in range(10000):\n",
    "    observation, info = env.reset()\n",
    "    total_rewards = 0\n",
    "    while True:\n",
    "        # Pick an action using the random Python module random until we have a valid action for that cell\n",
    "        # Hint: use states_actions_map in conjunction with the random module\n",
    "        current_state = observation\n",
    "        valid_actions = states_actions_map[current_state]\n",
    "        action = random.choice(valid_actions)\n",
    "\n",
    "        # Get a new observation, reward, terminated and other information here using the env.step() function\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Update rewards\n",
    "        total_rewards += reward\n",
    "\n",
    "        # break this loop if we have reached an end state\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    reward_list.append(total_rewards)\n",
    "\n",
    "# calculate and print the average reward here\n",
    "avg_reward = sum(reward_list) / len(reward_list)\n",
    "print(\"Average reward obtained: \", avg_reward)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward obtained:  -51.886\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "04b22b01-2c98-4ad3-b9c6-6f1d9416868d",
   "metadata": {},
   "source": [
    "Hope this lab helped you understand how to set up and navigate an environment in Gymnasium.\n",
    "As always, please don't hesitate to reach out to the instructor or the TAs if you have any questions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
